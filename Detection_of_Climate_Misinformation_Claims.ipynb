{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alizoljodi/Behavior_economy_analysis/blob/main/Detection_of_Climate_Misinformation_Claims.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb4acB3imHA_"
   },
   "source": [
    "Detection of Climate Misinformation Claims\n",
    "==================================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cydCLNANFiQk"
   },
   "source": [
    "# Project Description\n",
    "\n",
    "Climate change is one of the most pressing challenges of our time, yet public understanding is often undermined by the widespread circulation of misinformation. Detecting and classifying such misinformation at scale is therefore critical to safeguarding evidence-based climate communication.\n",
    "\n",
    "This project addresses the problem of **automatic classification of climate-related contrarian claims** into predefined sub-claim categories, as proposed in *Coan et al. (2021)*. The task is challenging due to the diversity, nuance, and evolving nature of misinformation narratives, coupled with the large number of possible subcategories.\n",
    "\n",
    "I begin with **exploratory data analysis** to understand the dataset’s distribution, class balance, and textual characteristics. Building on these insights, I move to a three-phase LLM-based approach:\n",
    "\n",
    "\n",
    "1. **Zero-Shot Prompting**: Establish a baseline without task-specific examples. Import a small LLM model (google/gemma-2-2b-it for this experience) and prompt to classify the input text into sub claim classes.\n",
    "2. **Few-Shot Prompting**: Incorporate curated in-context examples for improved accuracy alongside above test.\n",
    "3. **Advanced Methods**: Leverage state-of-the-art text classification methods such as Fine-tuning, Retrival Augmented Generation (RAG) and text embedding + fully connected classifications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-TZrqq9mnMR"
   },
   "source": [
    "For this project, I use **accuracy** to compare the results, but **precision**, **recall**, and **F1-score** are also reported.\n",
    "\n",
    "### Metrics and Formulas\n",
    "\n",
    "**Accuracy**  \n",
    "Accuracy = (Number of Correct Predictions) / (Total Number of Predictions)\n",
    "\n",
    "**Precision**  \n",
    "Precision = True Positives / (True Positives + False Positives)\n",
    "\n",
    "**Recall**  \n",
    "Recall = True Positives / (True Positives + False Negatives)\n",
    "\n",
    "**F1-Score**  \n",
    "F1-Score = 2 × (Precision × Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vl3Bzo3dTP1z"
   },
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FocrkN35TO2c",
    "outputId": "bde83b94-8083-418b-9ace-50557eecacd3"
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate evaluate datasets\n",
    "!pip -q install bitsandbytes pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H0jsRwzRWAjz",
    "outputId": "44469095-dabc-409e-feb8-8d18ee69513e"
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vz68aI2qWk8y",
    "outputId": "6cacb6be-ec4f-4d89-e051-8dec9d58730c"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --no-cache-dir sentencepiece\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pC7v_AnDRo76",
    "outputId": "5646dfc8-44fd-44bf-ada3-bf9a38193a4f"
   },
   "outputs": [],
   "source": [
    "!pip -q install -U sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfOiymtNa8IR",
    "outputId": "1b8ceb50-3909-4976-8826-b34e2b203c56"
   },
   "outputs": [],
   "source": [
    "!pip -q install -U fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_PmwhUXWqMB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)  # forces Colab restart so new install is active\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teTeRChHKQr9"
   },
   "source": [
    "### Import important modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irAKyfFfKQJd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#from wordcloud import WordCloud\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "from transformers import  TrainingArguments, DataCollatorForLanguageModeling, Trainer\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import math, json, gc\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IT4he0GdX5Z1"
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_VjFUaom9VK",
    "outputId": "75ea3efd-cfe9-4e56-9fef-49453e0ce969"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EB6mqDfq250"
   },
   "source": [
    "# Exploartoy Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPTa5ZrXHy4-"
   },
   "source": [
    "## Dataset Structure\n",
    " The dataset contains three .csv files, training.csv, validation.csv, and test.csv, which are prepared for training models, validating their performance, and testing the final results, respectively.\n",
    "\n",
    "\n",
    " Below, I perform exploratory data analysis on these three files to examine the number of samples, the distribution of labels, and the structure of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHqfZzaIVAYS"
   },
   "source": [
    "### Export important tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZA9AJmDaJt7l"
   },
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtsEyqPwH5st"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('training.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "validation_df = pd.read_csv('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhlRGxitOoxO"
   },
   "source": [
    "### Analysis the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "dOqymPC8KKYK",
    "outputId": "93052db2-f7e5-424b-f89a-e22644d80242"
   },
   "outputs": [],
   "source": [
    "# Get the number of rows from each DataFrame\n",
    "train_rows = len(train_df)\n",
    "test_rows = len(test_df)\n",
    "validation_rows = len(validation_df)\n",
    "\n",
    "# Data for the pie chart\n",
    "labels = ['Training', 'Test', 'Validation']\n",
    "sizes = [train_rows, test_rows, validation_rows]\n",
    "colors = ['#ff9999','#66b3ff','#99ff99']\n",
    "\n",
    "# Calculate total samples\n",
    "total_samples = sum(sizes)\n",
    "\n",
    "# Create labels with counts and percentages\n",
    "pie_labels = [f'{labels[i]}: {sizes[i]} ({sizes[i]/total_samples:.1%})' for i in range(len(labels))]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.pie(sizes, labels=pie_labels, colors=colors, autopct='', startangle=90)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.title('Distribution of Samples Across Different Sets out of {samples}'.format(samples=total_samples))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LHcXW2DO2Lw"
   },
   "source": [
    "The dataset contains 28,945 samples divided into three sets: a training set with 23,436 samples (81%), a validation set with 2,605 samples (9%), and a test set with 2,904 samples (10%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IA7x72UsRSZ4"
   },
   "source": [
    "### Dataset columns review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DS4PQQadRhp-"
   },
   "source": [
    "Due to their similarity, I review the columns only for the training set and later refer to the validation and test sets to examine their data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "FGhgnyMSOW2I",
    "outputId": "53a0a034-4145-40e8-e66a-59e481004e18"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPx2XzM6R_iR",
    "outputId": "9404714f-3419-41ef-cb55-c8fa2834f615"
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRJSlvdQSO7v"
   },
   "source": [
    "Each row of the dataset contains three columns:\n",
    "\n",
    "- **text** *(object)*: A textual description related to climate change that the model must process to determine whether it contains any valid claim about climate change.\n",
    "- **sub_claim_code** *(object)*: A code representing the valid claim and sub-claim category that the model must predict by analyzing the **text** input.\n",
    "- **sub_claim** *(object)*: A textual description of the claim category that the model must predict from the **text** input.\n",
    "\n",
    "All columns have the datatype **object**, and the dataset appears to be clean, containing no null values. This suggests that no data cleaning is required before proceeding with analysis and modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VbYISnhUNFY"
   },
   "source": [
    "### Data Description\n",
    "\n",
    "For all three sets, I examine their data descriptions to obtain basic information about their statistics, such as data redundancy, the most frequent values in each column, and their corresponding frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "141f77d6",
    "outputId": "1486908b-ad5b-4452-e9f9-22a92966bfb5"
   },
   "outputs": [],
   "source": [
    "# Run describe on each dataframe and store the results\n",
    "train_desc = train_df.describe(include='all')\n",
    "test_desc = test_df.describe(include='all')\n",
    "validation_desc = validation_df.describe(include='all')\n",
    "\n",
    "# Create a new dictionary to hold the restructured data\n",
    "restructured_data = {}\n",
    "\n",
    "# Iterate through the index of the describe output (the statistics)\n",
    "for index in train_desc.index:\n",
    "    restructured_data[index] = {\n",
    "        'train': train_desc.loc[index].to_dict(),\n",
    "        'test': test_desc.loc[index].to_dict(),\n",
    "        'validation': validation_desc.loc[index].to_dict()\n",
    "    }\n",
    "\n",
    "# Create a new DataFrame from the restructured data\n",
    "restructured_df = pd.DataFrame.from_dict({(i,j): restructured_data[i][j]\n",
    "                           for i in restructured_data.keys()\n",
    "                           for j in restructured_data[i].keys()},\n",
    "                          orient='index')\n",
    "\n",
    "# Display the restructured dataframe\n",
    "display(restructured_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeczqpV3Wl8P"
   },
   "source": [
    "The results of the data description suggest the following findings:\n",
    "\n",
    "- The dataset is clean with respect to redundancy: the training and validation sets contain no duplicate samples, and the test set contains only one duplicate, which is negligible.\n",
    "- The majority of labels in all three sets are `\"No claim\"`, accounting for approximately two-thirds of the samples in each set. This class imbalance suggests the need for caution, as it may bias the model’s predictions toward the majority class.\n",
    "- We also have 18 different labels to predict by model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tJjfZ5pY3Q4"
   },
   "source": [
    "### The distribution of labels in each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "da4ad1c5",
    "outputId": "66efc89c-c2d8-4b86-cb0f-d606572f6ae8"
   },
   "outputs": [],
   "source": [
    "# Get the value counts for each dataframe\n",
    "train_counts = train_df['sub_claim_code'].value_counts()\n",
    "test_counts = test_df['sub_claim_code'].value_counts()\n",
    "validation_counts = validation_df['sub_claim_code'].value_counts()\n",
    "\n",
    "# Create a figure with three subplots in a row\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Plot the distribution for the training set\n",
    "axes[0].bar(train_counts.index, train_counts.values)\n",
    "axes[0].set_title('Training Set')\n",
    "axes[0].set_xlabel('Sub-Claim')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "\n",
    "# Plot the distribution for the test set\n",
    "axes[1].bar(test_counts.index, test_counts.values)\n",
    "axes[1].set_title('Test Set')\n",
    "axes[1].set_xlabel('Sub-Claim')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Plot the distribution for the validation set\n",
    "axes[2].bar(validation_counts.index, validation_counts.values)\n",
    "axes[2].set_title('Validation Set')\n",
    "axes[2].set_xlabel('Sub-Claim')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].tick_params(axis='x', rotation=90)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JCannJ-ZAL4"
   },
   "source": [
    "The histograms of the three sets suggest that they share a similar data distribution, which indicates that validation and test set performance should provide a reliable representation of the model’s performance on the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_1d4tMba7Tj"
   },
   "source": [
    "## Analysis of Input Text\n",
    "\n",
    "In this section, I conduct an exploratory analysis of the input columns, examining features such as text length, number of words, most frequent words, and other relevant characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2Y2RtKlbmTS"
   },
   "source": [
    "### Text length analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "6VOAeboXbkKR",
    "outputId": "abd6bdad-a8aa-4c0e-9a82-d88c799936ce"
   },
   "outputs": [],
   "source": [
    "# Calculate the length of the text in each dataframe\n",
    "train_text_lengths = train_df['text'].str.len()\n",
    "test_text_lengths = test_df['text'].str.len()\n",
    "validation_text_lengths = validation_df['text'].str.len()\n",
    "\n",
    "# Create a figure with three subplots in a row\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Plot the histogram for the training set\n",
    "axes[0].hist(train_text_lengths, bins=50)\n",
    "axes[0].set_title('Training Set Text Length Distribution')\n",
    "axes[0].set_xlabel('Text Length')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot the histogram for the test set\n",
    "axes[1].hist(test_text_lengths, bins=50)\n",
    "axes[1].set_title('Test Set Text Length Distribution')\n",
    "axes[1].set_xlabel('Text Length')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Plot the histogram for the validation set\n",
    "axes[2].hist(validation_text_lengths, bins=50)\n",
    "axes[2].set_title('Validation Set Text Length Distribution')\n",
    "axes[2].set_xlabel('Text Length')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vn2uGcGrcZah"
   },
   "source": [
    "The text length distributions vary across the different sets. This variation may impact the performance of certain conventional methods, such as Long Short-Term Memory (LSTM) networks or Feed-Forward Neural Networks (FFNs), which can be sensitive to input length. In contrast, the effect on Large Language Models (LLMs) is expected to be minor, as they are generally more robust to variations in sequence length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzBcBlZvdOPS"
   },
   "source": [
    "### Word count analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "_sd7RWAMcIoS",
    "outputId": "28280a52-aa93-4732-8a31-e6c87c248a5c"
   },
   "outputs": [],
   "source": [
    "# Calculate the word count of the text in each dataframe\n",
    "train_word_counts = train_df['text'].str.split().str.len()\n",
    "test_word_counts = test_df['text'].str.split().str.len()\n",
    "validation_word_counts = validation_df['text'].str.split().str.len()\n",
    "\n",
    "# Create a figure with three subplots in a row\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Plot the histogram for the training set\n",
    "axes[0].hist(train_word_counts, bins=50)\n",
    "axes[0].set_title('Training Set Word Count Distribution')\n",
    "axes[0].set_xlabel('Word Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot the histogram for the test set\n",
    "axes[1].hist(test_word_counts, bins=50)\n",
    "axes[1].set_title('Test Set Word Count Distribution')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Plot the histogram for the validation set\n",
    "axes[2].hist(validation_word_counts, bins=50)\n",
    "axes[2].set_title('Validation Set Word Count Distribution')\n",
    "axes[2].set_xlabel('Word Count')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FfHYQsNeHkL"
   },
   "source": [
    "A similar observation applies to the word count distribution across the datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISeAhWQqfygC"
   },
   "source": [
    "### Wordcloud Representation of text column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "06zoFyynegOU",
    "outputId": "c5bc6105-09f5-4aa2-87a3-9ce239538633"
   },
   "outputs": [],
   "source": [
    "# Combine the text from all three dataframes\n",
    "all_text = pd.concat([train_df['text'], test_df['text'], validation_df['text']])\n",
    "\n",
    "# Join all text into a single string\n",
    "try:\n",
    "  combined_text = \" \".join(review for review in all_text)\n",
    "except:\n",
    "  combined_text = \" \".join(str(review) for review in all_text)\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(stopwords=None, background_color=\"white\").generate(combined_text)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Word Cloud of All Datasets')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9BN5bWHfuty"
   },
   "source": [
    "The wordcloud suggests that the most repetive words are global, climate, change, year, and warming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKvtQMh-egR4"
   },
   "source": [
    "# Evalaute different models performance in the text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8-68FfdWRHC"
   },
   "source": [
    "### Login to the hugging face\n",
    "To download models it is necessary to login to huggingface account or forward the authentication tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iduHNbGWL0b"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"YOUR_HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41D5oX_RvKlk"
   },
   "source": [
    "### Create a list of all possible answers to prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaMGLi8ggVv7"
   },
   "outputs": [],
   "source": [
    "# Get all unique values from the 'sub_claim' column in the training data\n",
    "sub_claim_labels = train_df['sub_claim'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvXWvDKewc_Q"
   },
   "source": [
    "## Zeroshot prompting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tam1K2lHvy1p"
   },
   "source": [
    "### Define model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SubhS01UKFQ"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"google/gemma-2-2b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq877Qd_wImr"
   },
   "source": [
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXHR71pSUmQ_"
   },
   "outputs": [],
   "source": [
    "USE_4BIT = False  # set True if using \"google/gemma-2-9b-it\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tv4faNtUv6og"
   },
   "outputs": [],
   "source": [
    "\n",
    "INPUT_COL = \"text\"\n",
    "BATCH_SIZE = 8\n",
    "OUTPUT_COL = \"sub_claim\"\n",
    "MAX_INPUT_TOKENS = 512\n",
    "MAX_NEW_TOKENS = 4\n",
    "CHECKPOINT_EVERY = 100\n",
    "OUT_PATH = \"predictions.parquet\"\n",
    "CSV_MIRROR = \"predictions.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clISVDeZv5vq"
   },
   "source": [
    "Set quantized weights to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSyV34OBUocm"
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True) if USE_4BIT else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5wI8LXswVbj"
   },
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "4b8013744347400895bdc2e9105151de",
      "536253f844ac4842b5542f5d3b4f4fb3",
      "50dd75ae6d6d4da6a6b4eba0e0cfc311",
      "7473e4270a1e4b4e9a6b117bbd291b38",
      "f5da2433e35a4780bf7709302dd80666",
      "ecbae41772324f34b99d979beba55081",
      "b610e34dc39349d5bb018d3cfd2eab8a",
      "b2e3a2d70bcf41af96bd46e4b2707069",
      "5edc833bd0734157b5e34e748f0cfd6c",
      "565cfdd1dc774f02af19cb26f3ba1764",
      "5e630b15c0aa42feb939685d96c2b7c8",
      "92dd031101a64a558c723c5f5f78c376",
      "29963af9d57e4fbabb92d0aa107cc39d",
      "25de674a3435440ea3db75c6d9341e11",
      "9ca2dc1f634b4a57b75a6480baaaa56c",
      "1a647470c0b343668cf38bef81d45064",
      "f731060f611e4bec8bca6e525aa58a35",
      "2b5714f0c4f243bf84913024b722483f",
      "82de52b272e74f54863fb6edf0d045e6",
      "cb96d1892c4e49319acd03380911eceb",
      "d2c835a08d0f4dfb8c75adb78c4a27f1",
      "5820a7f4fe454ce7b1b1f66be2df796b",
      "4a56c6b41f334ab0aae99620c94a42d6",
      "67184ee1f14c4024827668701ffe703f",
      "2a59a80202784cadbed1ec8c6809c1f9",
      "6a130456b1a043c1a785efa375158766",
      "c7577b4915404170b063ecfdf81c6e69",
      "f5e80083d9424bbf88e999635401c642",
      "427da81c0dcb4b1e8fff7d533d610332",
      "33ac926dd26e40fc82db431d51f57da2",
      "78dc81c1051d4d9fa308dbac720b1caf",
      "34063821900e4bc6959825e732603c30",
      "4cd82560054f449e96ff9ff147a63220",
      "253ececc6be34247a8d7612aa61c9cf5",
      "3ed15f479cae4c1f8c44ea9fbd8d93c0",
      "e40429160c6549ffae2a4ba6c3bf72c8",
      "b157f9da8d874f70add224814c86467f",
      "15153b7f9a454cafbfcfa788e72f4213",
      "a526d70d1f354396870616d7d1a6ce46",
      "81a68b114f394103bb6d959ce6080896",
      "256302e54cdb4bc78bf033d181d26923",
      "ff86104913f04744a82988ebae5b5bbe",
      "cb98b4736b4743b59216ed9302f4498f",
      "53e0f9bd16f342e29f0b6f925f5a9f4b",
      "7778c291d8184175ad911978e173d749",
      "dde0c10429fd4ba58d2b3a00b0a641ac",
      "23157790520e4d748df8a2284a57fd4f",
      "bc94ab087db34495857e649343efdabb",
      "1c11604564e243fcb66e6635697e2362",
      "bb9cc3649843428995bcaee7362370e8",
      "531b66a4780a4abab4c3454778482fca",
      "b7ed3208f2ca430ca3b35db658b6d33e",
      "75deeac1288d420198f256657e2d3bd2",
      "e2ab58b51005408d89fbee655834a2b5",
      "f6d9efe8c5404476a35aa185a85aa04e",
      "5acf25e792b2422ea3608d7c6594c9dc",
      "b00cfbbc36af4a5fbff5c3c8fd5741c3",
      "94c385de29fe4b81b86ff0976a82eec5",
      "418391760c01490dacec4ecc4d60cf5a",
      "57257363039a47b4abfa544adc20ba79",
      "ba974cddd012470ca4cb9c3599bfcd84",
      "49b6c13fecc24d098c3001962766e59c",
      "99879d89c5ca4980a0bafda0c3fc9dbb",
      "c200cea602704574891207a2f30fa9a9",
      "1697fe7cd01742e39b80bf1e350a8756",
      "9004640ce22a4a6ba5828308fd39448b",
      "a7467950dd724649a8e164d982b1f89e",
      "2e54392844a64de6a4d5ec9b2b0c501d",
      "a29070b2599b49a0b9346ec724abc151",
      "86a5526dff2a46b59153a8ca6dc37ac1",
      "e6d93106fa634f91bec0374f61b62776",
      "ce81dc2f37b8446ab14d912ba81fea24",
      "3d5aa90a682249169324d7de26db3604",
      "69f99e7c16c14c13a06973940feb17b6",
      "0e2c76c5551f4cd6b9ce5f4ae52e1274",
      "e67006fb4e48424983550fa63c466982",
      "9ffde616483f4a0699d0647011ca9c00"
     ]
    },
    "id": "946qI3WSUq2P",
    "outputId": "16f4218d-7805-41a9-97ae-4679cff0b793"
   },
   "outputs": [],
   "source": [
    "dtype = torch.float16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_O5sxjL4wkCx"
   },
   "source": [
    "### Define Tokenization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "1cf16fbd6cbe465393719ba0588ef85f",
      "3ed090b930d645f194eda9de4f316211",
      "e8eab89e421b4775afa2c0e9e1cad401",
      "7fe21bb304ec4e26badc1b632911f697",
      "dfb63a5cb92743ca98204022f6481c87",
      "24f0d25986be47f0b323a5979cf26d1e",
      "ff087f08f37343c7bc52562f286de101",
      "2819e6682c1a484d83a62ff260d02aeb",
      "d89dd4a287f14f5caa13c4ba2c941c7d",
      "a76545a01a6a4ebb990b34c79be46004",
      "736339f019af432f8b880ade2a61965f",
      "e446944ffaeb4af3ad59b12fcf7aa87b",
      "8c59066b28c94b5093ced04fbdbb8ad6",
      "bac7349b0ea541288787c13fbf3f38ff",
      "512c2720276b4ab5ade53ef7a89c5472",
      "0607a6bea488419099de084769529668",
      "c8597cd80c014be3ba1c5f2e857bedcb",
      "2d01fba2df3c4ea5b1ff10d1600cef5f",
      "c17cfaa494c8443998816e3f03f4a073",
      "4ad49cd0f15e45519babdf6ba9a5bb05",
      "e926c210c6e6442dbc7e388404537abf",
      "8403aec5b5814b3cac241092ff4c281b",
      "7c5251763b5e43ee914d607f0b5abae1",
      "6a3342799df94f66a47c4930f6b239a1",
      "a8692ec76b9743a2a753716240245e3e",
      "f732d727950c4364b466393c931dbd5b",
      "66269327eff644edbb4a962e099a9f8f",
      "ced1f46a196d4ab8a557026c18e5ab69",
      "8abc1a80fcdf49f88bd6e59391754ab2",
      "b9a42e3607154c36b923834e17639677",
      "320cb6d352924ed4bc00067f07909791",
      "7d849c1efcc84d58ab316c71b7dc5b2c",
      "6e16905e1d564c46bc6f343bfa75966e",
      "993db66ec3da409e8b2e6a1b05e784af",
      "f6bb2988622744d39bfb3bd5ec7c2871",
      "cc711a380fc2423b80609793f4d29b81",
      "0271ce25395b4eb6aef366d056ccaa2c",
      "1e7b97da5b3a4c41ae6a65721020bfbb",
      "a741c5d2b4c54af3975e2d3a991c2da1",
      "2341832d690747158468e81c4589f239",
      "d0aa422ac5d64be2a26d56bd8521f1a9",
      "2e6ff66757d046249ed90f184ff997b0",
      "db2edca0c45445f4be57384e6fc7e35c",
      "be1c291f93d2477fbe46b6385b0dc73e"
     ]
    },
    "id": "NBm4mWK-UtTB",
    "outputId": "7e44622e-c6f6-401f-e1bf-b2f9fe4a334f"
   },
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4QH7CgDwrgJ"
   },
   "source": [
    "### Prompt of Zeroshot results\n",
    "* You are a strict classifier. Possible labels: ***List of labels***. Return only one label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "457a3yjPYXCQ"
   },
   "outputs": [],
   "source": [
    "def format_prompt(x):\n",
    "    # Keep it short and force a closed set\n",
    "    return (\n",
    "        \"You are a strict classifier. \"\n",
    "        f\"Possible labels: {', '.join(sub_claim_labels)}.\\n\"\n",
    "        \"Return ONLY one label.\\n\\n\"\n",
    "        f\"TEXT:\\n{x}\\n\\nLabel:\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SuOrsV_xlTZ"
   },
   "source": [
    "### Parse labels\n",
    "* Empty answers assume to be \"No claim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUJzGO-FYaB4"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_label(text):\n",
    "    if not text or not text.strip():\n",
    "        return \"No claim\"\n",
    "    first = text.strip().splitlines()[0].strip().lower()\n",
    "    # optional: normalize punctuation/spaces\n",
    "    first_norm = re.sub(r'\\s+', ' ', first)\n",
    "\n",
    "    for lab in sub_claim_labels:\n",
    "        if first_norm.startswith(lab.lower()):\n",
    "            return lab\n",
    "\n",
    "    # fallback: look for any exact label mention\n",
    "    pat = r'\\b(' + '|'.join(re.escape(l.lower()) for l in sub_claim_labels) + r')\\b'\n",
    "    m = re.search(pat, first_norm)\n",
    "    return m.group(1) if m else \"No claim\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EAc82ZNxz7h"
   },
   "source": [
    "### Check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4W_7BacYiXX"
   },
   "outputs": [],
   "source": [
    "if not INPUT_COL in test_df.columns:\n",
    "    raise ValueError(f\"Missing column: {INPUT_COL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dT9sXY3x_bq"
   },
   "source": [
    "### Robustify code against crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61T7YVfFYny_"
   },
   "outputs": [],
   "source": [
    "done_idx = set()\n",
    "if os.path.exists(OUT_PATH):\n",
    "    prev = pd.read_parquet(OUT_PATH)\n",
    "    # Assuming original order; if you have a stable ID column, use that instead\n",
    "    done_idx = set(prev.index.tolist())\n",
    "    print(f\"Resuming: {len(done_idx)} rows already done.\")\n",
    "else:\n",
    "    prev = pd.DataFrame(index=[], columns=[OUTPUT_COL])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQZMEhbgYq4I",
    "outputId": "3ef9abfc-da10-4cc4-b64c-b44a6ec2fe75"
   },
   "outputs": [],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omz1pYICyLNJ"
   },
   "source": [
    "### Run the prompt on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCgcX52-YsY4"
   },
   "outputs": [],
   "source": [
    "def generate_batch(prompts):\n",
    "    enc = tok(\n",
    "        prompts, padding=True, truncation=True,\n",
    "        max_length=MAX_INPUT_TOKENS, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    input_lens = enc[\"attention_mask\"].sum(dim=1).tolist()  # true per-sample lengths\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            min_new_tokens=1,  # force at least one token\n",
    "            do_sample=False,\n",
    "            temperature=None,  # avoid the warning\n",
    "            eos_token_id=tok.eos_token_id or getattr(model.config, \"eos_token_id\", None),\n",
    "            pad_token_id=tok.pad_token_id or getattr(model.config, \"pad_token_id\", None),\n",
    "        )\n",
    "\n",
    "    decoded = []\n",
    "    for i, L in enumerate(input_lens):\n",
    "        # slice per-sample using its true input length\n",
    "        out_tokens = gen[i, L:] if gen.size(1) > L else gen.new_zeros((0,))\n",
    "        if out_tokens.numel() == 0:\n",
    "            decoded.append(\"\")   # will be handled by parse_label\n",
    "        else:\n",
    "            decoded.append(tok.decode(out_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATLvI95xyn5K"
   },
   "source": [
    "### Run to get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ecba4802a4f841a29e064a30daf03158",
      "58d186cd0d6044fdb48fad5170193dc9",
      "aaa7a26f2add4594a728ca8552a7aa67",
      "1a4f0ff56e9a4505ac9394c2048a179c",
      "be172518aeca487db4b89ac96cdd7fa9",
      "eb87c09fc8b94211b9eee49ec5b1ca92",
      "b587722c19dd499295b087f8b5105bb9",
      "907734d3de554b14bcd080dbc1809ca7",
      "99181f2684904ff3b9de1e02722d4681",
      "bc9a730ae96c4fddb5f08b29a3acd10a",
      "3248624fa4c64a07a93dab41671b3eb1"
     ]
    },
    "id": "pZxdIV7ZY3r_",
    "outputId": "c6cb83aa-16dc-4099-dde6-a1738581636a"
   },
   "outputs": [],
   "source": [
    "results = prev.copy()\n",
    "if results.empty:\n",
    "    results = pd.DataFrame(index=test_df.index, columns=[OUTPUT_COL])\n",
    "todo_indices = [i for i in test_df.index if i not in done_idx]\n",
    "print(f\"Remaining: {len(todo_indices)}\")\n",
    "for start in tqdm(range(0, len(todo_indices), BATCH_SIZE)):\n",
    "    batch_idx = todo_indices[start:start+BATCH_SIZE]\n",
    "    texts = [test_df.at[i, INPUT_COL] for i in batch_idx]\n",
    "    prompts = [format_prompt(t) for t in texts]\n",
    "\n",
    "    try:\n",
    "        gens = generate_batch(prompts)\n",
    "        labels = [parse_label(g) for g in gens]\n",
    "        results.loc[batch_idx, OUTPUT_COL] = labels\n",
    "    except RuntimeError as e:\n",
    "        # Handle occasional CUDA OOM\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            torch.cuda.empty_cache()\n",
    "            # retry with smaller batch\n",
    "            for i in batch_idx:\n",
    "                try:\n",
    "                    g = generate_batch([prompts[batch_idx.index(i)]])[0]\n",
    "                    results.at[i, OUTPUT_COL] = parse_label(g)\n",
    "                except Exception:\n",
    "                    results.at[i, OUTPUT_COL] = \"error\"\n",
    "            continue\n",
    "        else:\n",
    "            # mark errors and continue\n",
    "            for i in batch_idx:\n",
    "                results.at[i, OUTPUT_COL] = \"error\"\n",
    "            continue\n",
    "    finally:\n",
    "        # Periodic checkpoint\n",
    "        if (start // BATCH_SIZE) % (CHECKPOINT_EVERY // max(BATCH_SIZE,1)) == 0:\n",
    "            results.to_parquet(OUT_PATH)\n",
    "            results.to_csv(CSV_MIRROR, index=True)\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sDmbi95z0Fd"
   },
   "source": [
    "### Evaluate the Zeroshot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "s6mkLeNEZCQC",
    "outputId": "f835252e-19e6-4a7b-b9d2-96b931cd14a5"
   },
   "outputs": [],
   "source": [
    "# Load predictions from the CSV file\n",
    "predictions_df = pd.read_csv(\"/content/predictions.csv\", index_col=0)\n",
    "\n",
    "# Ensure the indices align\n",
    "predictions_df = predictions_df.reindex(test_df.index)\n",
    "\n",
    "# Get the true labels and predicted labels\n",
    "y_true = test_df['sub_claim']\n",
    "y_pred = predictions_df[OUTPUT_COL]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "# Create a DataFrame to display the results with the specified structure\n",
    "evaluation_metrics = pd.DataFrame({\n",
    "    'Model': ['gemma 2b'],\n",
    "    'Accuracy': [accuracy],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1-Score': [f1]\n",
    "}, index=['Zero shot'])\n",
    "\n",
    "# Display the results\n",
    "display(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4CizWbZfEz5"
   },
   "source": [
    "The zero-shot performance is not acceptable. Possible reasons include the specificity of the task, meaning the pre-trained LLM was not trained on similar data, and the relatively small size of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbqOTMBX079D"
   },
   "source": [
    "## Few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTF6gDsV3ZgS"
   },
   "outputs": [],
   "source": [
    "OUT_PATH           = \"predictions_fewshot.parquet\"\n",
    "CSV_MIRROR         = \"predictions_fewshot.csv\"\n",
    "\n",
    "FEWSHOT_K          = 3                        # number of examples per prompt\n",
    "MAX_EX_TEXT_CHARS  = 280                      # truncate each example text (pre-tokenization)\n",
    "MAX_INPUT_TOKENS   = 1024                     # full prompt cap (few-shot + test)\n",
    "MAX_NEW_TOKENS     = 4                        # classification -> tiny output\n",
    "BATCH_SIZE         = 4                        # careful with VRAM; reduce if OOM\n",
    "CHECKPOINT_EVERY   = 100                      # rows\n",
    "SEED               = 42                       # reproducibility (sampling)\n",
    "LABELS_OVERRIDE    = None                     # e.g. [\"pos\",\"neg\",\"neu\"] or None to infer from train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rn-ht1gN8mDp"
   },
   "outputs": [],
   "source": [
    "def truncate_text_chars(s, max_chars):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    return s[:max_chars] if max_chars and len(s) > max_chars else s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5X6tfPXJ79AT"
   },
   "outputs": [],
   "source": [
    "def build_few_shot_prompt(train_df, k, input_col, label_col, test_text, labels, seed=None):\n",
    "    \"\"\"Randomly sample k examples from train_df and build a few-shot prompt.\"\"\"\n",
    "    if seed is not None:\n",
    "        # different seed per call to avoid identical samples each row\n",
    "        rnd = random.Random(seed + random.randint(0, 1_000_000))\n",
    "        sampled = train_df.sample(n=min(k, len(train_df)), random_state=rnd.randint(0, 1_000_000))\n",
    "    else:\n",
    "        sampled = train_df.sample(n=min(k, len(train_df)))\n",
    "\n",
    "    parts = [\n",
    "        \"You are a strict text classifier.\",\n",
    "        f\"Possible labels: {', '.join(labels)}.\",\n",
    "        \"Return ONLY the label name.\\n\"\n",
    "    ]\n",
    "    for _, r in sampled.iterrows():\n",
    "        ex_text = truncate_text_chars(r[input_col], MAX_EX_TEXT_CHARS)\n",
    "        parts.append(f\"TEXT: {ex_text}\\nLabel: {r[label_col]}\\n\")\n",
    "\n",
    "    parts.append(f\"TEXT: {truncate_text_chars(test_text, MAX_EX_TEXT_CHARS)}\\nLabel:\")\n",
    "    return \"\\n\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzeQ_7Ny8NmR"
   },
   "outputs": [],
   "source": [
    "def atomic_save(df):\n",
    "    tmp = OUT_PATH + \".tmp\"\n",
    "    df.to_parquet(tmp)\n",
    "    os.replace(tmp, OUT_PATH)\n",
    "    df.to_csv(CSV_MIRROR, index=True, na_rep=\"PENDING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIiYGcUq8P2y"
   },
   "outputs": [],
   "source": [
    "def generate_batch(prompts):\n",
    "    enc = tok(\n",
    "        prompts, padding=True, truncation=True,\n",
    "        max_length=MAX_INPUT_TOKENS, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    input_lens = enc[\"attention_mask\"].sum(dim=1).tolist()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            min_new_tokens=1,           # ensure at least one token\n",
    "            do_sample=False,            # deterministic for classification\n",
    "            temperature=None,           # avoid \"ignored\" warning\n",
    "            eos_token_id=tok.eos_token_id or getattr(model.config, \"eos_token_id\", None),\n",
    "            pad_token_id=tok.pad_token_id or getattr(model.config, \"pad_token_id\", None),\n",
    "        )\n",
    "\n",
    "    decoded = []\n",
    "    for i, L in enumerate(input_lens):\n",
    "        out_tokens = gen[i, L:] if gen.size(1) > L else gen.new_zeros((0,))\n",
    "        if out_tokens.numel() == 0:\n",
    "            decoded.append(\"\")\n",
    "        else:\n",
    "            decoded.append(tok.decode(out_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8Td2JHp8SJq",
    "outputId": "52db86b6-843b-4a1c-d12b-0c2009a2d808"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(OUT_PATH):\n",
    "    results = pd.read_parquet(OUT_PATH)\n",
    "    # Make sure it aligns with current test_df\n",
    "    missing_idx = set(test_df.index) - set(results.index)\n",
    "    if missing_idx:\n",
    "        # expand to include missing rows\n",
    "        extra = pd.DataFrame(index=sorted(list(missing_idx)))\n",
    "        results = pd.concat([results, extra], axis=0).sort_index()\n",
    "    if \"prediction\" not in results.columns:\n",
    "        results[\"prediction\"] = pd.NA\n",
    "    done_mask = results[\"prediction\"].notna()\n",
    "    print(f\"Resuming: {done_mask.sum()} already done / {len(test_df)}\")\n",
    "else:\n",
    "    results = pd.DataFrame(index=test_df.index, columns=[\"prediction\"])\n",
    "    done_mask = results[\"prediction\"].notna()\n",
    "    print(f\"Starting fresh: {len(test_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTob7vX68Vri"
   },
   "outputs": [],
   "source": [
    "todo_indices = [i for i in test_df.index if not done_mask.loc[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3d15b78c75364a4e91784e51b307b3bb",
      "7962fdd61bde4317bd1ffbfaf6da3898",
      "27431283536e45a997f694e9254e5d35",
      "8ab25328c36d43f889cbc1c72b7c4733",
      "56336d47694742408306642ec8c487c5",
      "a65608e3b35c4f33a1c8159ea9e084a4",
      "23793de978354fc2929a0e91f9167d40",
      "c71e766a9ee143508a7aa914fcdda01b",
      "4d884eb5a239484b89db7b3aa3ea25a9",
      "bcf16bb4025746fcab015181e9381106",
      "ebddb7581205405e8f4a8242434fde29"
     ]
    },
    "id": "fcjU7W_g8ZYj",
    "outputId": "b01b8381-81f0-4383-fb0c-2015a88d23f2"
   },
   "outputs": [],
   "source": [
    "for start in tqdm(range(0, len(todo_indices), BATCH_SIZE)):\n",
    "    batch_idx = todo_indices[start:start + BATCH_SIZE]\n",
    "    batch_texts = [test_df.at[i, INPUT_COL] for i in batch_idx]\n",
    "\n",
    "    # Build a few-shot prompt per test text (each gets its own random k-shot)\n",
    "    prompts = [\n",
    "        build_few_shot_prompt(\n",
    "            train_df=train_df,\n",
    "            k=FEWSHOT_K,\n",
    "            input_col=\"text\",\n",
    "            label_col=\"sub_claim\",\n",
    "            test_text=txt,\n",
    "            labels=sub_claim_labels,\n",
    "            seed=SEED\n",
    "        )\n",
    "        for txt in batch_texts\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "\n",
    "        gens = generate_batch(prompts)\n",
    "        labels = [parse_label(g) for g in gens]\n",
    "        results.loc[batch_idx, \"prediction\"] = labels\n",
    "        print(results)\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            torch.cuda.empty_cache()\n",
    "            # retry one-by-one\n",
    "            for i, prompt in zip(batch_idx, prompts):\n",
    "                try:\n",
    "                    g = generate_batch([prompt])[0]\n",
    "                    results.at[i, \"prediction\"] = parse_label(g)\n",
    "                except Exception:\n",
    "                    results.at[i, \"prediction\"] = \"error\"\n",
    "        else:\n",
    "            for i in batch_idx:\n",
    "                results.at[i, \"prediction\"] = \"error\"\n",
    "    finally:\n",
    "        # periodic checkpoint\n",
    "        processed = len(results[\"prediction\"].dropna())\n",
    "        if processed % CHECKPOINT_EVERY < BATCH_SIZE:\n",
    "            atomic_save(results)\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "# Final save\n",
    "atomic_save(results)\n",
    "print(\"Saved:\", OUT_PATH, \"and\", CSV_MIRROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "SmEF0qRRt_bl",
    "outputId": "6cffbf89-f90c-4e35-bad9-e0fc8ff4cc01"
   },
   "outputs": [],
   "source": [
    "# Load predictions from the CSV file\n",
    "predictions_df = pd.read_csv(\"/content/predictions_fewshot.csv\", index_col=0)\n",
    "\n",
    "# Ensure the indices align\n",
    "predictions_df = predictions_df.reindex(test_df.index)\n",
    "\n",
    "# Get the true labels and predicted labels\n",
    "y_true = test_df['sub_claim']\n",
    "y_pred = predictions_df[\"prediction\"]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "# Create a DataFrame to display the results with the specified structure\n",
    "few_shot_metrics = pd.DataFrame({\n",
    "    'Model': ['gemma 2b'],\n",
    "    'Accuracy': [accuracy],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1-Score': [f1]\n",
    "}, index=['Few Shot'])\n",
    "\n",
    "# Append the few-shot metrics to the existing evaluation_metrics DataFrame\n",
    "evaluation_metrics = pd.concat([evaluation_metrics, few_shot_metrics])\n",
    "\n",
    "# Display the results\n",
    "display(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cHrwZ9bfuG5"
   },
   "source": [
    "Few-shot prompting performs slightly better than zero-shot, but the results are still below the acceptable threshold. The improvement can likely be attributed to the inclusion of example samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2U5FwHllvEV0"
   },
   "source": [
    "### Efficient Fine-Tuning with QLoRA and PEFT\n",
    "QLoRA enables efficient fine-tuning of large language models by quantizing weights to 4-bit precision while preserving model quality. PEFT (Parameter-Efficient Fine-Tuning) methods, such as LoRA, update only a small set of additional parameters, reducing memory usage and training time. Combined, they allow low-resource hardware to fine-tune large models effectivelly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrBnpNp8mx-R"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME   = \"google/gemma-2-2b-it\"   # or \"google/gemma-2-2b\" if you prefer\n",
    "TEXT_COL     = \"text\"\n",
    "LABEL_COL    = \"sub_claim\"\n",
    "MAX_LEN      = 384\n",
    "EPOCHS       = 3\n",
    "LR           = 2e-4\n",
    "BATCH_TRAIN  = 2        # T4-friendly; raise if you can\n",
    "BATCH_EVAL   = 2\n",
    "GRAD_ACCUM   = 8\n",
    "OUTPUT_DIR   = \"./gemma2_cls_qlora\"\n",
    "SEED         = 42\n",
    "SUBSET_SIZE  = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gclmyT88nBJV"
   },
   "outputs": [],
   "source": [
    "train_df = train_df[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)\n",
    "test_df  = test_df[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRqrRCfhnMt7",
    "outputId": "14999deb-7a28-452f-8d88-551a36b64c7b"
   },
   "outputs": [],
   "source": [
    "labels = sorted(pd.unique(train_df[LABEL_COL].tolist() + test_df[LABEL_COL].tolist()))\n",
    "label2id = {l:i for i,l in enumerate(labels)}\n",
    "id2label = {i:l for l,i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjVQpI7wnPXW"
   },
   "outputs": [],
   "source": [
    "train_hf = Dataset.from_pandas(train_df)\n",
    "validation_hf  = Dataset.from_pandas(validation_df)\n",
    "test_hf = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2woFixfqnTpg"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89u8dAGqnWP2"
   },
   "outputs": [],
   "source": [
    "compute_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "b1b1e4909e1c49b7bafd327fe240de0e",
      "dfa4c720b7854372a86966e437f8e532",
      "bd21ddccd8ab42018d5da9f3663f80c1",
      "e52bf857ec044a419b29deff50b09a4f",
      "fef8c67f69104b318888dde293359a8f",
      "8b84ee08cc7243dea998f28837fc82f7",
      "dfe67940639b45eaa7e9b19284d7d11e",
      "061a7c0a15b54f7481142053861c0263",
      "783480d2c20845f280fae963c2a77d98",
      "47ae05b8626b42f1bf55671f85f4cc5a",
      "d020712a35654852a238f4a662303f6c"
     ]
    },
    "id": "O8qazIR0nYhU",
    "outputId": "c38a8083-010f-4b7f-a944-b33c9a2896a2"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    attn_implementation=\"eager\",   # recommended for Gemma 2\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIBJw26QtWHc"
   },
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJ-f_odxnbJN"
   },
   "outputs": [],
   "source": [
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azCkzl-Ongf4"
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_cfg)\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OC1dh5BBnipI"
   },
   "outputs": [],
   "source": [
    "label_tokens = {}\n",
    "space_id = tokenizer.encode(\" \", add_special_tokens=False)[0]\n",
    "for lab in labels:\n",
    "    ids = tokenizer.encode(\" \" + str(lab), add_special_tokens=False)\n",
    "    label_tokens[lab] = ids[0] if len(ids)>0 else tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7M-MGpTtj2a",
    "outputId": "8716d8d9-8ccf-4cec-c1ef-d2f54283da1a"
   },
   "outputs": [],
   "source": [
    "trainable, total = 0, 0\n",
    "for n, p in model.named_parameters():\n",
    "    total += p.numel()\n",
    "    if p.requires_grad:\n",
    "        trainable += p.numel()\n",
    "print(f\"Trainable params: {trainable/1e6:.2f}M / {total/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ru2no-cpguDR"
   },
   "source": [
    "**The prompt:**  \n",
    "You are a helpful classifier.  \n",
    "Choose one label from the list.  \n",
    "\n",
    "**Text:**  \n",
    "...  \n",
    "\n",
    "**Answer:**  \n",
    "(Respond with just the label.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpuxUgksnkro"
   },
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = (\n",
    "    \"You are a helpful classifier. \"\n",
    "    \"Choose one label from: {label_list}.\\n\\n\"\n",
    "    \"Text:\\n{text}\\n\\n\"\n",
    "    \"Answer with just the label.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a65m47UYnmHs"
   },
   "outputs": [],
   "source": [
    "def build_sample(example):\n",
    "    text = str(example[TEXT_COL])\n",
    "    lab  = str(example[LABEL_COL])\n",
    "    prompt = PROMPT_TEMPLATE.format(label_list=\", \".join(labels), text=text)\n",
    "\n",
    "    enc = tokenizer(prompt, truncation=True, max_length=MAX_LEN, padding=False)\n",
    "    input_ids = enc[\"input_ids\"]; attn = enc[\"attention_mask\"]\n",
    "\n",
    "    # append \" space + label_token\" for supervised next-token learning\n",
    "    label_id = label_tokens[lab]\n",
    "    input_ids_with_label = input_ids + [space_id, label_id]\n",
    "    attn_with_label = attn + [1,1]\n",
    "\n",
    "    # supervise only the final label token; ignore prompt tokens with -100\n",
    "    labels_arr = [-100]*len(input_ids) + [-100, label_id]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_with_label,\n",
    "        \"attention_mask\": attn_with_label,\n",
    "        \"labels\": labels_arr,\n",
    "        \"gold_label\": lab,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "99687655c6a84daea24aa490996c7d57",
      "5904916bb04b4915b17e29a90b1f88ee",
      "72a268608b854f1d84b91f13793f06a7",
      "44642e17c9194c66b284978b2f1cbb00",
      "43b33e0cf28c474397144114986d6571",
      "24b19aa775884b5f89ed62175450a7b4",
      "89d6e66fc64e4b83a708fa640f9293c2",
      "f9962f8da1b94102b91d3c96494b824d",
      "d85910bf611743bbad87791831bf15e7",
      "a15119095b334b51a99449d4670989a2",
      "68bf507ceb4d414bb41fc477a9520d0d",
      "3b9ad53e43c7488b99754792f66c0051",
      "66961d839f544cc1a717d4ebeb6212a7",
      "2eaf0bcc3f0c42938df9796a9c3efbf7",
      "4eec739a7fb5449d99cdad8e959845f6",
      "7a9c40d903f54a75a64257de090cb500",
      "f4922a525e764bdc958b81594c83b8d9",
      "7818c41952374cbb86cf720ecf272858",
      "4a938be11c6d44e39a2b441481606755",
      "4252e4f8502946b4a99464a97bd04d24",
      "b968e8a1139e49a99b852007574e2267",
      "59ec0aaaf32b4c58a52c1045e07900ee",
      "1b4d45d2e3594c74915dba262527b77c",
      "77f1182a0b7540718a1fc486b009e36d",
      "c74e187854104461b9da72fe50c3d579",
      "ad87b141061c42f4a7398caba5ab61d7",
      "5a3689f7f9f84e7ab56a92755f28c387",
      "c95dc6a1107742c1831f1730b5654af5",
      "26cc6cdc8e534540b8613fd3dfed1c1d",
      "867d9cf32dad4557a34700dde4f927de",
      "2f86c3cde17e493dace67f5dd8908c46",
      "2b25dcca0eab4d32af6cda51a5849121",
      "f79863c53b0d42da809ee5226c756972"
     ]
    },
    "id": "rXGzcrqPnoGz",
    "outputId": "c53c702e-13f1-4381-cc81-1eb0a3b5b858"
   },
   "outputs": [],
   "source": [
    "train_hf = train_hf.map(build_sample, remove_columns=train_hf.column_names)\n",
    "validation_hf= validation_hf.map(build_sample, remove_columns=validation_hf.column_names)\n",
    "test_hf  = test_hf.map(build_sample,  remove_columns=test_hf.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qj_s-aCnpy0"
   },
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    padded = tokenizer.pad(\n",
    "        {\n",
    "            \"input_ids\": [b[\"input_ids\"] for b in batch],\n",
    "            \"attention_mask\": [b[\"attention_mask\"] for b in batch],\n",
    "        },\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    max_len = padded[\"input_ids\"].size(1)\n",
    "\n",
    "    labels_pad = []\n",
    "    for b in batch:\n",
    "        l = b[\"labels\"]\n",
    "        if len(l) < max_len:\n",
    "            l = l + [-100] * (max_len - len(l))\n",
    "        labels_pad.append(l)\n",
    "    padded[\"labels\"] = torch.tensor(labels_pad, dtype=torch.long)\n",
    "\n",
    "    # only add gold_label if present in ALL items\n",
    "    if all(\"gold_label\" in b for b in batch):\n",
    "        padded[\"gold_label\"] = [b[\"gold_label\"] for b in batch]\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEZ6tLHWn3et"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_labels(eval_dataset):\n",
    "    model.eval()\n",
    "    preds, golds = [], []\n",
    "    device = model.device\n",
    "    for ex in eval_dataset:\n",
    "        # prompt without appended \" space + label\"\n",
    "        inp  = ex[\"input_ids\"][:-2]\n",
    "        attn = ex[\"attention_mask\"][:-2]\n",
    "        # ask the model to predict next token after adding a space\n",
    "        inp2  = inp + [space_id]\n",
    "        attn2 = attn + [1]\n",
    "\n",
    "        tens   = torch.tensor([inp2], device=device)\n",
    "        attn_t = torch.tensor([attn2], device=device)\n",
    "        out = model(input_ids=tens, attention_mask=attn_t)\n",
    "        next_logits = out.logits[0, -1]  # distribution for next token\n",
    "\n",
    "        # score each label by its first token logit\n",
    "        scores = [next_logits[label_tokens[lab]].item() for lab in labels]\n",
    "        pred_lab = labels[int(np.argmax(scores))]\n",
    "        preds.append(pred_lab)\n",
    "        golds.append(ex[\"gold_label\"])\n",
    "    return preds, golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2foBmWwMn5Qk"
   },
   "outputs": [],
   "source": [
    "def compute_metrics_on(eval_dataset):\n",
    "    y_pred, y_true = predict_labels(eval_dataset)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision\": pr, \"recall\": rc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibbFjWy4n7IO"
   },
   "outputs": [],
   "source": [
    "class SubsetTrainer(Trainer):\n",
    "    def __init__(self, *args, subset_size=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Training requires a train_dataset.\")\n",
    "        if self.subset_size is not None:\n",
    "            sampler = RandomSampler(\n",
    "                self.train_dataset, replacement=True, num_samples=self.subset_size\n",
    "            )\n",
    "        else:\n",
    "            sampler = self._get_train_sampler()\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.train_batch_size,\n",
    "            sampler=sampler,\n",
    "            collate_fn=self.data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNA1lPKLn9qr"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    logging_steps=25,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    bf16=(compute_dtype==torch.bfloat16),\n",
    "    fp16=(compute_dtype==torch.float16),\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K2XoXW3Sn_7i",
    "outputId": "f7a6a392-b53c-4621-91dc-3df3e826f6ec"
   },
   "outputs": [],
   "source": [
    "trainer = SubsetTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_hf,\n",
    "    eval_dataset=validation_hf,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate,\n",
    "    subset_size=SUBSET_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "jEWMnEUwoDh2",
    "outputId": "ae397413-b00f-412a-a580-8d977c052ed2"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "W954iV8LoGx6",
    "outputId": "062498d7-00ae-47a0-c2d4-945c7342ee01"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_metrics = compute_metrics_on(test_hf)\n",
    "\n",
    "# Create a DataFrame with the test metrics\n",
    "peft_qlora_results = pd.DataFrame({\n",
    "    'Model': ['gemma 2b'],\n",
    "    'Accuracy': [test_metrics['accuracy']],\n",
    "    'Precision': [test_metrics['precision']],\n",
    "    'Recall': [test_metrics['recall']],\n",
    "    'F1-Score': [test_metrics['f1']]\n",
    "}, index=['PEFT QLORA'])\n",
    "\n",
    "# Append the PEFT QLORA metrics to the existing evaluation_metrics DataFrame\n",
    "evaluation_metrics = pd.concat([evaluation_metrics, peft_qlora_results])\n",
    "\n",
    "# Display the results\n",
    "display(evaluation_metrics)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "peft_qlora_results.to_csv(\"PEFT_QLORA_results.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n39YYVWwhJO7"
   },
   "source": [
    "The fine-tuning results are significantly better, likely because updating a subset of the model’s weights provides it with task-specific knowledge, enabling better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdXdHHNo5yHJ"
   },
   "source": [
    "### Using **Chain-of-Thought (COT) + Self-Consistency**\n",
    "I use Chain-of-Thought (CoT) prompting to guide the model through step-by-step reasoning before producing the final answer. I also apply Self-Consistency, where the model generates multiple reasoning paths for the same query and the most frequent final answer is selected, improving reliability and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOboYP3u6Gbt"
   },
   "outputs": [],
   "source": [
    "EVAL_SAMPLES = 300\n",
    "SC_N = 3                # self-consistency passes\n",
    "MAX_NEW_TOKENS = 64\n",
    "TEMP = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EE0fTRCc6ers"
   },
   "outputs": [],
   "source": [
    "labels = sorted(pd.unique(pd.concat([train_df[LABEL_COL], test_df[LABEL_COL]], ignore_index=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8R6Q0TlS6tz5"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "34f44a3f6a6443e5a9c442d4e7023fba",
      "f9b0c7146e7941f4bda84c95f02aa362",
      "723c54471d71489f8556996cf3a4a3c6",
      "f742711e9ce0467c9be32ab51bbd6468",
      "ca19b22a097f4b618873575ff4a8b884",
      "7d41e2c5619a465380aedce758e3da93",
      "daed9ecdbfda463994566003d618a400",
      "45a27b30f23c481d902fec4cb2d35ba5",
      "f5ba2ba9dc7e4153b0b12d9593b5e090",
      "3bc638b267cf40fb82b0dc6dea35398e",
      "1e11f9c1549b473db36f5bf5833e88a0"
     ]
    },
    "id": "WbWmGNOy7DcI",
    "outputId": "0e388814-8185-428d-8ed2-ffd4ba781dac"
   },
   "outputs": [],
   "source": [
    "compute_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "model.eval()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NokD49dsiL0D"
   },
   "source": [
    "**Prompt**\n",
    "You are a careful fact/claim classifier.  \n",
    "Possible labels: <label1>, <label2>, <label3>  \n",
    "\n",
    "Instructions:  \n",
    "1) Think step by step about the text and its meaning.  \n",
    "2) On the last line, output exactly: Label: <one of the labels>  \n",
    "\n",
    "Text:  \n",
    "<text>  \n",
    "\n",
    "Reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kv2PGE9S7GEY"
   },
   "outputs": [],
   "source": [
    "def cot_prompt(text, labels):\n",
    "    return (\n",
    "        \"You are a careful fact/claim classifier.\\n\"\n",
    "        f\"Possible labels: {', '.join(labels)}\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"1) Think step by step about the text and its meaning.\\n\"\n",
    "        \"2) On the last line, output exactly: Label: <one of the labels>\\n\\n\"\n",
    "        f\"Text:\\n{text}\\n\\n\"\n",
    "        \"Reasoning:\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ka6ppM-c7LXP"
   },
   "outputs": [],
   "source": [
    "def parse_label(output, labels):\n",
    "    m = re.search(r\"Label:\\s*([^\\n\\r]+)\", output, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        cand = m.group(1).strip()\n",
    "        # pick the first exact label that appears\n",
    "        for L in labels:\n",
    "            if re.search(rf\"\\b{re.escape(L)}\\b\", cand, flags=re.IGNORECASE):\n",
    "                return L\n",
    "    # fallback: first label mentioned anywhere\n",
    "    for L in labels:\n",
    "        if re.search(rf\"\\b{re.escape(L)}\\b\", output, flags=re.IGNORECASE):\n",
    "            return L\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyEC0Lzx7NtI"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def classify_cot_once(model, tokenizer, text, labels, max_new_tokens=64, temperature=0.7):\n",
    "    prompt = cot_prompt(text, labels)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=(temperature > 0),\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    out = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "    return parse_label(out, labels), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74xb4zZk7QAx"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def classify_self_consistency(model, tokenizer, text, labels, n=3, max_new_tokens=64, temperature=0.7):\n",
    "    votes = []\n",
    "    for _ in range(n):\n",
    "        pred, _ = classify_cot_once(model, tokenizer, text, labels,\n",
    "                                    max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "        if pred is not None:\n",
    "            votes.append(pred)\n",
    "    if not votes:\n",
    "        return None\n",
    "    # majority vote\n",
    "    return max(set(votes), key=votes.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDNyWX2D7R8t"
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "subset_idx = rng.choice(len(test_df), size=min(EVAL_SAMPLES, len(test_df)), replace=False)\n",
    "eval_df = test_df.iloc[subset_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "y2J9MXwQ7Vqk",
    "outputId": "174b500b-59d8-4009-cc42-95a52b894d62"
   },
   "outputs": [],
   "source": [
    "y_true, y_pred = [], []\n",
    "for i, row in eval_df.iterrows():\n",
    "    pred = classify_self_consistency(\n",
    "        model, tokenizer,\n",
    "        row[TEXT_COL],\n",
    "        labels,\n",
    "        n=SC_N,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=TEMP\n",
    "    )\n",
    "    y_true.append(row[LABEL_COL])\n",
    "    # safe default if parsing failed (rare)\n",
    "    y_pred.append(pred if pred is not None else labels[0])\n",
    "    if (i+1) % 50 == 0:\n",
    "        print(f\"Processed {i+1}/{len(eval_df)}\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "# Create a DataFrame to display the results with the specified structure\n",
    "cot_sc_metrics = pd.DataFrame({\n",
    "    'Model': ['gemma 2b'],\n",
    "    'Accuracy': [accuracy],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1-Score': [f1]\n",
    "}, index=['COT + SC'])\n",
    "\n",
    "# Append the COT + SC metrics to the existing evaluation_metrics DataFrame\n",
    "evaluation_metrics = pd.concat([evaluation_metrics, cot_sc_metrics])\n",
    "\n",
    "# Display the results\n",
    "display(evaluation_metrics)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "cot_sc_metrics.to_csv(\"cot_sc.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgIdOFFgiX-R"
   },
   "source": [
    "The results are completely zero, indicating that this method may not be suitable for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPVU8EV4Rbzo"
   },
   "source": [
    "### Retrival Augmented Generation (RAG)\n",
    "Is a technique that combines information retrieval with text generation.  \n",
    "It first retrieves relevant documents or passages from an external knowledge source, such as a database or search index,  \n",
    "and then uses a language model to generate responses based on both the retrieved context and the original query.  \n",
    "This approach improves factual accuracy and allows the model to incorporate up-to-date or domain-specific information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mglig8R7YzU"
   },
   "outputs": [],
   "source": [
    "EMB_MODEL = \"intfloat/e5-base-v2\"\n",
    "TOP_K = 3\n",
    "MAX_EX_CHARS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337,
     "referenced_widgets": [
      "6701502f34a043d1b5ed6414d6dc0427",
      "a76521615e084f91bad53d45c3174e22",
      "5b00dd973e314cea94e2e185d65a9425",
      "e144bd43fba245b18e81987d063cfe34",
      "abf8ddbc2f7247f0a56416f6271b65bd",
      "fb89bbc1f60147e7beee4fc252c37296",
      "4ec1361f8c1f4798bf1f6cf82250e767",
      "4ffe9cad72204db09688d88172a60bfa",
      "7f8ca9775898452dac799236a35e7a98",
      "aa8202f6c7f249d685aacd0d5e6ebf38",
      "633456b0b5024631ba5c033e2c62ee24",
      "5226d6b05e2e4ed0adca2b9825d847f0",
      "d67881a2ab0f4862b37fab2811815cad",
      "e1e43e0cf01e452b9f8025f98e46d209",
      "bcd3b7220a014fa28f6d10d7a90e7c44",
      "2256850ac5394117841589b73c0da973",
      "b5946afcea5849b3b31422c64b0ea987",
      "81d1f61232814e9ba2f823872d8ca477",
      "9c4f31332e5b49218e5055f72992d7c8",
      "867ad089f6a94f208ae516c53023e3b7",
      "f1770b22cd7740a396b29f754e2f9653",
      "290aa7d9d32c401081ea1cfe940d316e",
      "43c4b00badd94967b6a2e0cd3ea4ef7b",
      "f14978a6adea43a4988031ce9f240b34",
      "0c75d17757124320ad5942874a5b1045",
      "7c5fd912a92442b89847790ea9f7be1e",
      "62395385e5ca4a44916a83448dd5eee5",
      "dcfd52f9b3204631810f3fdb72bc9881",
      "b68914c3244c43f7a2419fedc9662100",
      "c2db707274484307a93b70ec331b7c90",
      "fbfbfb2b138c46d8be2b36ba555c3b89",
      "9a0ca15b2f78429ca4850e0cef602432",
      "3bea2b63f1ff4c05bff415a41d130310",
      "b92df68a1ab04fd99b78b9d6bee3e931",
      "e7d59e1cb7254af5ada6f2feeefe89b8",
      "1d14a4808b004c50a028027b41e0b405",
      "d3969cd60fce42e49a68f9c38b379805",
      "105341b4290849b5a20ca2737fe9ee12",
      "c1cdf090590d4393ba20e2ec3c3eefc0",
      "fad9c6bc782a4d0983627705f3f3d499",
      "87fee299c1ac4d3095a500cfab042ea5",
      "7dc499f93199407e95998fab7b1c0f46",
      "662b10e220374c589ac29645b2b374ee",
      "e23a1060a3844f6bb4d7a24bdd518345",
      "6d103c7dbdac4b1683d858e61deaea2d",
      "83ddc2e5f4bd4aa28bf988acf9c9a59d",
      "3c21cc2ed22b4dfc99c5fc2d7de711d7",
      "c9656873fc1246efbbef846012c08fa8",
      "8ab268373c1843068ce929ece244d224",
      "387af8c41db6437d8f874966bcda6256",
      "d0e5b2307fed453b9b36b0af7ab49d91",
      "7ebfec2306f74c968b0c1c761ba513d9",
      "3e261e0afba6497cb941f1a4dd2b3cda",
      "d4754a16d6ed43539be66ef4d269e5b5",
      "15a4a30dfbb448e2bea473f9205bfe0a",
      "b040400fed23414a837f224bb798d50f",
      "a7a53476913d4528ad71d21aa31f41a2",
      "e28cffb856a74d8d93d41a33883b606a",
      "2c0574da2d4d424892938c0a50804981",
      "ebf2464bbf3e467a94ccfe03f1756754",
      "679c75640e0942b9aa248703d56fa4cc",
      "ac31d30856f84daaa77a1fd9cf38db29",
      "ca83740cf0634a49bd5351a22f645686",
      "6c14a8f042444a308f1a98e345f9250b",
      "308be291336d4bfea60bc66d19a4aa2a",
      "62b3f10e80534bb58fc86cf2756de211",
      "b88969bf70894419a1adc5bc0714ea96",
      "f86cc170843c40aa9b3fbcd8b5971ad0",
      "9224033059dc42b9a44aea4e9074c21a",
      "7fb34122de5c403f8e664b698713ff05",
      "cc86a24a5a8840af9377226a18b8e0f5",
      "195b6f30d2094f83814e9dec3c67f2de",
      "cef889804a9743228ef2350b2d0e5b0e",
      "d62ffa532513435992f64fc1fef89744",
      "2adf835d94ed44bd885da1b7e5a08136",
      "3af962bd2d2a4d65b5fb4dbd6bea9c3e",
      "beee029b35074b94aca73efac7e472c8",
      "b7659e0ddc0c4c52ae662b8d2bcae45b",
      "d4357aa129bc445cb5928e3557e615d4",
      "06411945a70345ca8ad120a847ada590",
      "d658652deb104152947ee7308b154b26",
      "573f86feb8a84bd0b1e251e5a541a358",
      "aba51018b28946efb112e3f4f14c92c0",
      "26c467958511414188fda02626fb359e",
      "05b2a995f7074a89bc93e2333d41d953",
      "98cf2a4f822741029e1dc975f85cd459",
      "4700479377e5492aa01348ec6e9016ba",
      "97990340d84c4f40b04e95e7c2e1b3b4",
      "7b7e819e04fb496bbc21ae1c5f5a814a",
      "1b50d7a558ce4e68a080633a2b6b61bf",
      "d60beaaa46af4ce39e72d27b2cba5a54",
      "d5aeb1f7497441b6ac1c0f33e86db29a",
      "9afdefcc1c37442f8db7a18636e7a1ca",
      "25a96aaf09ea4c7d9f4aa58df930e78f",
      "9028945273d24ecc8223cc260a73656d",
      "26b57be411514c18927e6d7f2d990bad",
      "8612e5bbdb3844abb0505f440fedc3f6",
      "59993c80730144d895fe7175040d9fed",
      "0a1348b65cf447e2ba92273975a92794",
      "bc6f1c03e7a6409dbcee75749b6492d3",
      "aa5c492bf07447528059c6b43e79a3ba",
      "ccea0ce85b6d4b7593bce73ac94db56e",
      "08bcaeb9109947f88f2565170daa9e26",
      "e5689c5cf87e47079229703c1705e21a",
      "98317c2314254c99b16456c6bde79660",
      "af749124c8504ed8acbee6529bbb64ea",
      "b282e9a1f24a40acaa763be182fd77b1",
      "42c45bfd519d4545aacf22e828de0316",
      "69ea44c687d04169815d0832c07a3ce7",
      "74a5d5d40f8a403191c502117d7af4a7"
     ]
    },
    "id": "ijf9EWFRSHq7",
    "outputId": "4b9da874-a712-452f-df09-511449884b50"
   },
   "outputs": [],
   "source": [
    "emb = SentenceTransformer(EMB_MODEL, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9GsWWuhSJk5"
   },
   "outputs": [],
   "source": [
    "def embed_passages(texts, batch_size=256):\n",
    "    return emb.encode([f\"passage: {t}\" for t in texts],\n",
    "                      batch_size=batch_size, show_progress_bar=True,\n",
    "                      convert_to_numpy=True, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMVy-rgVSOIW"
   },
   "outputs": [],
   "source": [
    "train_texts = train_df[TEXT_COL].astype(str).tolist()\n",
    "train_labels = train_df[LABEL_COL].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9b09aedf78d84cceb95dc1dfb3c06f45",
      "5b1fabadb34a4fbbb543d102196f5a16",
      "70188f15221b4445a567da6d91e9cd6c",
      "8f8259e8b6034865947753c5b0da9f2b",
      "98dd872e91e9455a9d84a092c65fd095",
      "d4845d53dff349ec910dca6385020c81",
      "89eb2691cb1b4beaa47da23be758f9fc",
      "eef0141b74ee4bfd9bb9abde22e3c2ac",
      "a15078dc96e7469285be53166af9dddd",
      "6d409216207548ff98e2485879cfac0e",
      "4157a0595bde489595baa09a1c18ee5b"
     ]
    },
    "id": "NYLabwGdSP6b",
    "outputId": "80150c5d-66a0-4827-cdea-1392a93f26ca"
   },
   "outputs": [],
   "source": [
    "X = embed_passages(train_texts)          # (N, d) unit-normalized\n",
    "index = faiss.IndexFlatIP(X.shape[1])    # cosine via dot product on normalized vecs\n",
    "index.add(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SliG4OASTiW"
   },
   "outputs": [],
   "source": [
    "def retrieve_examples(query_text, k=TOP_K):\n",
    "    qv = emb.encode([f\"query: {query_text}\"],\n",
    "                    convert_to_numpy=True, normalize_embeddings=True)\n",
    "    D, I = index.search(qv, k)\n",
    "    out = []\n",
    "    for idx in I[0]:\n",
    "        t = train_texts[idx]\n",
    "        l = train_labels[idx]\n",
    "        if len(t) > MAX_EX_CHARS:\n",
    "            t = t[:MAX_EX_CHARS] + \"…\"\n",
    "        out.append((t, l))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzjGiah0Scno"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "e5f8a485c2c84f03912afdb7d6b26af6",
      "8510bbf436704c5592bb7f0d8ece0a4c",
      "59e4a35775494af0a4c64410bcd05a65",
      "afc9a5722a5b497aa3d84f7c672dad63",
      "46586d20214149018cea6e47bcb1bff2",
      "687e03caa7e44740a794e3b7e26e78a4",
      "09268021167146438fb004f1758284ec",
      "69f6e5098f774b94b076af18bfdb7fa5",
      "0e1b8f7157f24c7fa7015980cd19bbc9",
      "cd39bdd3a4ca47ad816768acca7a3604",
      "bb918a32690747efaa1f5fb42f93f6c0"
     ]
    },
    "id": "mn24lPRZSfAq",
    "outputId": "2e7b091b-d57d-4716-f040-ec49ce73f9bb"
   },
   "outputs": [],
   "source": [
    "dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    attn_implementation=\"eager\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrqTZm0ISiyN"
   },
   "outputs": [],
   "source": [
    "model.eval(); model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxXipWqgSnxj"
   },
   "outputs": [],
   "source": [
    "def make_fewshot_block(examples):\n",
    "    # Short, labeled shots only\n",
    "    shots = []\n",
    "    for i, (t, l) in enumerate(examples, 1):\n",
    "        shots.append(f\"Example {i}\\nText: {t}\\nLabel: {l}\\n----\")\n",
    "    return \"\\n\".join(shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9QrAgnvSqcE"
   },
   "outputs": [],
   "source": [
    "def prompt_no_cot(query_text, labels, examples):\n",
    "    fewshot = make_fewshot_block(examples)\n",
    "    return (\n",
    "        \"You are a classifier. \"\n",
    "        f\"Possible labels: {', '.join(labels)}.\\n\\n\"\n",
    "        f\"{fewshot}\\n\\n\"\n",
    "        f\"Text: {query_text}\\n\"\n",
    "        \"Answer with exactly: Label: <one of the labels>\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cR3UU7TeSsfv"
   },
   "outputs": [],
   "source": [
    "def parse_label(output, labels):\n",
    "    m = re.search(r\"Label:\\s*([^\\n\\r]+)\", output, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        cand = m.group(1).strip()\n",
    "        for L in labels:\n",
    "            if re.fullmatch(re.escape(L), cand, flags=re.IGNORECASE):\n",
    "                return L\n",
    "        # fallback: contains\n",
    "        for L in labels:\n",
    "            if re.search(rf\"\\b{re.escape(L)}\\b\", cand, flags=re.IGNORECASE):\n",
    "                return L\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DbcqQa4Sup4"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def classify_rag_nocot(text, labels, max_new_tokens=16):\n",
    "    examples = retrieve_examples(text, k=TOP_K)\n",
    "    prompt = prompt_no_cot(text, labels, examples)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,               # deterministic\n",
    "        temperature=None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    out = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "    lab = parse_label(out, labels)\n",
    "    return lab if lab is not None else labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7dJL7PbSxDY"
   },
   "outputs": [],
   "source": [
    "EVAL_SAMPLES = min(500, len(test_df))\n",
    "idx = np.random.default_rng(42).choice(len(test_df), size=EVAL_SAMPLES, replace=False)\n",
    "eval_df = test_df.iloc[idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7MF6vLWSz_c"
   },
   "outputs": [],
   "source": [
    "y_true = eval_df[LABEL_COL].tolist()\n",
    "y_pred = [classify_rag_nocot(t, labels) for t in eval_df[TEXT_COL].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "qfPHYqUmS2dD",
    "outputId": "d073db9d-240c-4895-83b2-fb8eca26cd12"
   },
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "# Create a DataFrame to display the results with the specified structure\n",
    "rag_metrics = pd.DataFrame({\n",
    "    'Model': ['gemma 2b'],\n",
    "    'Accuracy': [accuracy],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1-Score': [f1]\n",
    "}, index=['RAG'])\n",
    "\n",
    "# Append the RAG metrics to the existing evaluation_metrics DataFrame\n",
    "evaluation_metrics = pd.concat([evaluation_metrics, rag_metrics])\n",
    "\n",
    "# Display the results\n",
    "display(evaluation_metrics)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "rag_metrics.to_csv(\"rag_results.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jc6WzKakJ8q"
   },
   "source": [
    "The accuracy of RAG is higher than that of fine-tuning the model, suggesting its strong potential for addressing the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtjVNLPoXZJG"
   },
   "source": [
    "### TF/IDF embedding combined to fully connected network\n",
    "\n",
    "**TF-IDF (Term Frequency–Inverse Document Frequency)** is a statistical method used to represent text as numerical vectors.  \n",
    "It assigns higher weights to terms that appear frequently in a document but less frequently across the corpus,  \n",
    "capturing words that are most relevant for distinguishing between documents.\n",
    "\n",
    "TF-IDF is important for text classification because it transforms unstructured text into meaningful, fixed-size  \n",
    "feature vectors that preserve important information while reducing the influence of common, less informative words.\n",
    "\n",
    "We combine TF-IDF embeddings with a **fully connected neural network** to leverage the strengths of both methods:  \n",
    "TF-IDF provides a robust, interpretable representation of the text, while the fully connected network learns complex,  \n",
    "non-linear decision boundaries for accurate classification.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0g3W5qvWWrBR"
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 100_000     # cap vocab size for speed/memory\n",
    "NGRAM_RANGE = (1, 2)       # unigrams + bigrams often help\n",
    "BATCH_SIZE  = 256\n",
    "LR          = 1e-3\n",
    "EPOCHS      = 20\n",
    "PATIENCE    = 3            # early stopping patience\n",
    "HIDDEN1     = 1024\n",
    "HIDDEN2     = 256\n",
    "DROPOUT     = 0.3\n",
    "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "utJ7lohnYByf",
    "outputId": "b24ed001-7ddc-4d25-eb0e-7d14879aa13c"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train_df[\"sub_claim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oike2U6RYGHC"
   },
   "outputs": [],
   "source": [
    "y_train = le.transform(train_df[\"sub_claim\"].astype(str))\n",
    "y_val   = le.transform(validation_df[\"sub_claim\"].astype(str))\n",
    "y_test  = le.transform(test_df[\"sub_claim\"].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lkfd8jHHYRdx",
    "outputId": "8e751b96-98a8-45f5-8da7-55e74848278a"
   },
   "outputs": [],
   "source": [
    "num_classes = len(le.classes_)\n",
    "print(\"Classes:\", list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxmJ6AaCYTSm",
    "outputId": "11d97494-2b89-403f-a09e-09d9fdcc6b5d"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=MAX_FEATURES, ngram_range=NGRAM_RANGE, lowercase=True)\n",
    "X_train = tfidf.fit_transform(train_df[\"text\"].astype(str))\n",
    "X_val   = tfidf.transform(validation_df[\"text\"].astype(str))\n",
    "X_test  = tfidf.transform(test_df[\"text\"].astype(str))\n",
    "input_dim = X_train.shape[1]\n",
    "print(f\"TF-IDF shape: train {X_train.shape}, val {X_val.shape}, test {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DmS2w6NYduh"
   },
   "outputs": [],
   "source": [
    "class SparseTfidfDataset(Dataset):\n",
    "    def __init__(self, X_csr, y_np):\n",
    "        self.X = X_csr\n",
    "        self.y = y_np\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        # convert 1 row CSR -> dense float32 only for this sample\n",
    "        x = torch.from_numpy(self.X[idx].toarray().ravel().astype(np.float32))\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6vuzUY0Yh6K"
   },
   "outputs": [],
   "source": [
    "train_ds = SparseTfidfDataset(X_train, y_train)\n",
    "val_ds   = SparseTfidfDataset(X_val, y_val)\n",
    "test_ds  = SparseTfidfDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jUaI_LjYjln"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjxREq6AYlFx"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, HIDDEN1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(HIDDEN1, HIDDEN2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(HIDDEN2, n_classes),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIpvcItIYnAq"
   },
   "outputs": [],
   "source": [
    "model = MLP(input_dim, num_classes).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSOMGM2uYoWA"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2To65jERYrl9"
   },
   "outputs": [],
   "source": [
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    losses, preds_all, labels_all = [], [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(DEVICE, non_blocking=True)\n",
    "        yb = yb.to(DEVICE, non_blocking=True)\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        preds_all.append(torch.argmax(logits, dim=1).detach().cpu().numpy())\n",
    "        labels_all.append(yb.detach().cpu().numpy())\n",
    "    y_pred = np.concatenate(preds_all)\n",
    "    y_true = np.concatenate(labels_all)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    return np.mean(losses), {\"accuracy\":acc, \"precision\":p, \"recall\":r, \"f1\":f1}, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3mpamJTYYvw5",
    "outputId": "692ff275-1c36-4e63-c3bc-48015b74dfb5"
   },
   "outputs": [],
   "source": [
    "best_val_f1, best_state, patience_left = -1.0, None, PATIENCE\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_metrics, _, _ = run_epoch(train_loader, train=True)\n",
    "    val_loss, val_metrics, _, _ = run_epoch(val_loader, train=False)\n",
    "    scheduler.step(val_metrics[\"f1\"])\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train loss {tr_loss:.4f} f1 {tr_metrics['f1']:.4f} | \"\n",
    "          f\"val loss {val_loss:.4f} f1 {val_metrics['f1']:.4f}\")\n",
    "    # Early stopping on val F1\n",
    "    if val_metrics[\"f1\"] > best_val_f1 + 1e-4:\n",
    "        best_val_f1 = val_metrics[\"f1\"]\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        patience_left = PATIENCE\n",
    "    else:\n",
    "        patience_left -= 1\n",
    "        if patience_left == 0:\n",
    "            print(\"Early stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLowr362YyLl"
   },
   "outputs": [],
   "source": [
    "test_loss, test_metrics, yt_true, yt_pred = run_epoch(test_loader, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "ImWOq80CZX4W",
    "outputId": "b1433e7f-bea9-4343-f78f-4ce970035732"
   },
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(yt_true, yt_pred)\n",
    "precision = precision_score(yt_true, yt_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(yt_true, yt_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(yt_true, yt_pred, average='weighted', zero_division=0)\n",
    "\n",
    "# Create a DataFrame to display the results with the specified structure\n",
    "tfidf_fnn_metrics = pd.DataFrame({\n",
    "    'Accuracy': [accuracy],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1-Score': [f1]\n",
    "}, index=['FFN+TFIDF'])\n",
    "\n",
    "# Append the TF/IDF+FNN metrics to the existing evaluation_metrics DataFrame\n",
    "evaluation_metrics = pd.concat([evaluation_metrics, tfidf_fnn_metrics])\n",
    "\n",
    "# Display the results\n",
    "display(evaluation_metrics)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "tfidf_fnn_metrics.to_csv(\"TF_IDF_FNN_results.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mduMvulkkzq"
   },
   "source": [
    "TF-IDF + FNN is the best-performing model for the task, achieving 79% accuracy.  \n",
    "This suggests that, in some cases, conventional methods can outperform more complex LLM-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSeU9UAflY11"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The results show a clear performance gap between conventional methods and zero/few-shot LLM approaches for this specific classification task.  \n",
    "Zero-shot and few-shot prompting with **Gemma 2B** performed poorly, likely due to the model's small size and lack of domain-specific training data.  \n",
    "**CoT + SC** failed to produce meaningful outputs, suggesting that reasoning-based prompting was not effective in this context.\n",
    "\n",
    "Fine-tuning with **PEFT QLoRA** significantly improved results, achieving over 71% accuracy, while **RAG** further increased performance to 73.8%,  \n",
    "demonstrating the benefit of augmenting the model with relevant retrieved context.  \n",
    "However, the best-performing approach was the **TF-IDF + Fully Connected Neural Network**, achieving **79.2% accuracy**.  \n",
    "This indicates that, for certain domain-specific text classification tasks, conventional machine learning methods can outperform more  \n",
    "complex and resource-intensive LLM-based solutions."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMia7IXVl8RSQ+DXz3jsbyz",
   "collapsed_sections": [
    "vl3Bzo3dTP1z",
    "teTeRChHKQr9",
    "ZA9AJmDaJt7l",
    "tam1K2lHvy1p"
   ],
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
